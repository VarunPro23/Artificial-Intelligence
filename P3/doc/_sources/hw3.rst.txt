AAIRobots 3 - Reinforcement Learning
==================================

Welcome to the AAIRobots Programming Project 3 for CSE 571 - Fall 2024!

This section will test your understanding of Reinforcement Learning as covered in the class.


=============================
Late Submission Policy
=============================

Please refer to the syllabus on canvas for the late submission policy. 
Please check gradescope for the due dates and plan your submission early.


=================
Grading
=================

1a. [20 points] Implement a function to compute Q-values (get_q_value) using Q-learning update rules in :code:`qlearning.py`. 

1b. [20 points] Implement a function for iteratively computing cumulative reward (compute_cumulative_reward) in :code:`qlarning.py`.

1c. [20 points] Implement a function for computing a decayed value for epsilon  (get_epsilon) in :code:`qlearning.py`. The decay routine should reduce the current epsilon value by 1%. The minimum value that the function returns must be 0.01.

2a. [30 points] Complete the implementation of :code:`learn()` function in :code:`qlearning.py`. In this function you are supposed to compute a policy to organize books using Q-learning. Use an \epsilon-greedy policy, which selects at each state, the best action according to current Q values with probability 1 - \epsilon and selects a random action with probability \epsilon. Start with Q values as zero for all state-action pairs.
    
2b. [10 points] Run the program with the q-value and decayed epsilon value computed and plot the resulting episode vs. reward plots.



Follow the instructions strictly to ensure that your assignment can be graded by the auto-grader. 
Name the zipped file as instructed. **Custom grading requests will not be entertained**.


Instructions
=================

===================================
Setting up **HW3** Folder
===================================
We assume that you have completed the setup for previous assingments and provided "hw3" repository is placed at the instructed location.

#. Change permission of all scripts to make them executable and install pip.

    .. code-block:: bash
    
       chmod u+x ~/catkin_ws/src/hw3/scripts/*.py && chmod u+x ~/catkin_ws/src/hw3/*.sh

#. Execute the env_setup.sh script. It will copy all the necessary files in respective folders. This script will fail if you don't have turtlebot folder in ~/catkin_ws/src. Refer Homework 0 setup if this is the case.

    .. code-block:: bash

       ~/catkin_ws/src/hw3/env_setup.sh && pip install tqdm

=============================
Environment Setting
=============================

**Environment 1 - bookWorld**

Refer to the image below to see how a sample maze environment for bookWorld looks like. 
The turtleBot has a basket on top of it. There are 4 books. Each book has a subject type and a size. There are 4 destination bins lying around in the environment. Each bin can accept books of a particular subject type and a particular size. 

.. figure:: ./images/books_n_bins.png
        :scale: 40 %
        :alt: A sample maze

**Environment 2 - cafeWorld**

The image below shows an example of the cafe environment that the turtlebot3 robot operates in. There are two foods and 4 tables.

.. figure:: ./images/food_n_tables.png
        :scale: 40 %
        :alt: A sample maze

Some of the terms that we use throughout the assignment for both the environments are:

#. **Objects and Goals:** In bookWorld, the objects are books, and the goals are bins. In cafeWorld, the objects are foods, and the goals are tables.

#. **Types of Objects:** In bookWorld, there are books of two subject types. In cafeWorld, there are foods of two food types.

#. **Sizes of Objects and Goals:** In bookWorld, there are books of two sizes - Large and Small, and bins of two sizes - Large and Small. In cafeWorld, there are foods of two sizes - Large and Small, and table of just one size.

#. **Number of Object Types:** The number of distinct object types can be either 1 or 2 in both environments. 

#. **Number of Objects:** This is the total number of objects of each type and each size in an environment. For each object type, an object of each size is created. E.g. for subject1 type in bookWorld, two books (one Large, one Small) are generated. So total number of objects in an environment is number of object types * number of objects per object type * number of sizes of object. The number of sizes of object is fixed 2 for both the environments.

#. **Number of Goals:** The total number of goals generated is number of object types * number of sizes of object. E.g. For subject1 type in bookWorld, two bins (one Large, one Small) are generated as goals, and for a food type in cafeWorld, two tables are generated as goals.

#. **Load Locations:** Load locations are the locations of turtlebot from which it can pick up an object.

	.. note::
		Grid Size is not used explicitly in this homework. It is dependent on the number of books in bookWorld and unused with cafeWorld.

..
	.. warning::
		We gave multiple load locations because it is possible that one of the object's load location is obstructed by another object. Same can also happen for the goal location. In such a case the TurtleBot can go to the another load location and perform the pick or place operation so that it can clear the path.

		**It is possible that both the load locations of a object or bin are obstructed.** In such a case no solution exists, and you have to regenerate a new problem. Reducing the # of objects avoids this problem.

=============================
Basic setup
=============================

#. Run the following to see basic help commands.

    .. code-block:: bash

        rosrun hw3 qlearning.py -h
        usage: qlearning.py [-h] [--objtypes OBJTYPES] [--objcount OBJCOUNT]
                            [--seed SEED] [--alpha ALPHA] [--gamma GAMMA]
                            [--episodes EPISODES] [--max-steps MAX_STEPS]
                            [--epsilon-task EPSILON_TASK] [--file-name FILE_NAME]
                            [--env [bookWorld/cafeWorld]] [--clean] [--submit]

        optional arguments:
          -h, --help            show this help message and exit
          --objtypes OBJTYPES   No of object types
          --objcount OBJCOUNT   The number of objects for each type in the environment.
          --seed SEED           The random seed
          --alpha ALPHA         The learning rate.
          --gamma GAMMA         The discount factor.
          --episodes EPISODES   The total number of episodes.
          --max-steps MAX_STEPS
                                  The max. steps per episode.
          --epsilon-task EPSILON_TASK
                                  The epsilon task to use.
          --file-name FILE_NAME
                                  Store results in <file> in the project root directory.
          --env [bookWorld/cafeWorld]
                                  Choose environment b/w cafeWorld and bookWorld
          --clean               Truncate output file
          --submit              Run submission for the epsilon-task

=============================
Task 1
=============================

Your task is to complete the following three functions in :code:`qlearning.py`

#. Complete the **get_q_value()** function for computing Q-values using Q-learning update rules.
#. Complete the **compute_cumulative_reward()** function for iteratively calculating cumulative reward.
#. Complete the **get_epsilon()** function for computing decaying \epsilon. 


=============================
Task 2 
=============================

#. Complete **learn()** function in :code:`qlearning.py`. Implement the overall subroutine to perform q-learning. Use an \epsilon-greedy policy, which selects at each state, the best action according to current Q values with probability 1 - \epsilon and selects a random action with probability \epsilon. Start with Q values as zero for all state-action pairs

#. Run the following command to generate the required submission files. 
    .. code-block:: bash  

        rosrun hw3 qlearning.py --submit


#. You should be able to see a progress bar in the terminal. If you do not see a pogress bar within few mins of running the rosrun command, run the following command to kill the running processes and rerun the above rosrun command.
    
    .. code-block:: bash

        ~/catkin_ws/src/hw3/hw3_kill.sh

#. Use the automatically generated **qlearning.csv** file to create a plot of episodes (x-axis) v/s cumulative reward for each environment. Name the plot **qlearning_{cafeWorld/bookWorld}.png**. 

.. note:: 
    You can use **test_qlearning.csv** in the project directory to validate your implementation. 


=============================
Submission configuration
=============================

    .. code-block:: bash

        objtypes=1
        objcount=1
        seed=100
        alpha=0.3
        gamma=0.9
        episodes=500
        max_steps=500

Both tasks 1 & 2 will use the same configuration.

========================
Submission Instructions
========================

Once ready to submit, verify all the files are generated (qlearning.py, qlearning.csv, and plots). Simply push the changes using the following commands: 

    .. code-block:: bash 

        git add scripts/qlearning.py
        git add *.csv
        git add *.png
        git commit -m "final commit"
        git push origin main


=============================
Environment Visualization
=============================

#.  Unlike the previous two assignments, this homework does not contain the visualization through Gazebo. However, you can view the environment generated by running the following command from the root of the directory after running `qlearning.py`

    .. code-block:: bash

        gazebo worlds/maze.sdf

=============================
Tips and Suggestions
=============================

#. You only need look at hw3_task.py (and the slides) to finish this task!
#. The provided validator files can help you verify your solution. Note however that some corner cases could be missing.
#. It does take a fair bit of time to run 500 episodes for two environments so plan on finishing it well before the deadline.

=============================
Watch it learn
=============================

Although there is no visualization through Gazebo, we can leverage ROS topics to view the status of the robot in real time.

While `qlearning.py` runs, open a new terminal and run the following command:

    .. code-block:: bash

        rostopic echo /status

This will print a stream of action related information from the robot directly to the console as the it executes the next best action. Notice the no-ops (failure to transition to a different state) and reason how this might affect its cumulative reward in an episode. 

If your implementation is correct, you will see the number of steps (actions) taken per episode reduce with sufficient episodes.

Episodes are delimited by the string 

    .. code-block:: bash

        +++++++++++++++++++++Reset world++++++++++++++++++++++

The `action_config.json` file contains the actions available to the robot and the corresponding rewards.



=============================
API
=============================

This API only lists the part of code that you will have to use to complete this project. 

As part of this project, you will only to modify :code:`qlearning.py`

===========================
class Helper
===========================
.. moduleauthor:: AAIR Lab, ASU <aair.lab@asu.edu>

.. automodule:: problem
    :noindex:

.. autoclass:: Helper
    :members: 

